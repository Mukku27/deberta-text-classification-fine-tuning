{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca707f3-4b06-4a1b-a99e-e4f9f4530cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract model weights and data\n",
    "!wget https://huggingface.co/sergak0/sn32/resolve/main/deberta-v3-large-hf-weights.zip\n",
    "!wget https://huggingface.co/sergak0/sn32/resolve/main/deberta-large-ls03-ctx1024.pth\n",
    "#!unzip /content/deberta-v3-large-hf-weights.zip -d deberta-v3-large-hf-weights #unzip not working in jupyter nb\n",
    "!wget https://huggingface.co/sergak0/sn32/resolve/main/data.zip\n",
    "#!unzip /content/data.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ce0ca-a014-4c37-9968-8a5c44c9878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the paths to the ZIP files and their respective extraction directories\n",
    "zip_file_paths = {\n",
    "    \"deberta-v3-large-hf-weights.zip\": \"./deberta-v3-large-hf-weights\",\n",
    "    \"data.zip\": \"./data\"\n",
    "}\n",
    "\n",
    "for zip_file_path, extract_to_dir in zip_file_paths.items():\n",
    "    # Create the extraction directory if it doesn't exist\n",
    "    os.makedirs(extract_to_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if the ZIP file exists\n",
    "    if os.path.exists(zip_file_path):\n",
    "        # Open and extract the ZIP file\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to_dir)\n",
    "        print(f\"Files from {zip_file_path} have been extracted to: {extract_to_dir}\")\n",
    "    else:\n",
    "        print(f\"File {zip_file_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74f792-0f6a-4d6c-a62d-0094d344a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch transformers scikit-learn pandas tqdm tiktoken tokenizers huggingface_hub sentencepiece gc-python-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4ddf7-97f6-46ae-8c90-bc43dc58aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c788b-9df6-445a-87a1-97f74fbcc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch==2.2.2\n",
    "!pip uninstall -y torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318c711-0fe7-4dd3-93ef-ec33e40fb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1528dc13-0f8f-43b1-a154-4b60bad2d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/env/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n",
      "Using device: cuda\n",
      "Using 2 GPUs!\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69dfea6ff2b44a1801ee748a68200fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/65491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52001ad1116045e48ab6391c6bc5f3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1724 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved! Metrics:\n",
      "avg_score: 0.9979\n",
      "f1: 0.9971\n",
      "fp_score: 0.9973\n",
      "ap: 0.9993\n",
      "auc: 0.9996\n",
      "val_loss: 0.0145\n",
      "\n",
      "Validation Metrics:\n",
      "avg_score: 0.9979\n",
      "f1: 0.9971\n",
      "fp_score: 0.9973\n",
      "ap: 0.9993\n",
      "auc: 0.9996\n",
      "val_loss: 0.0145\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, average_precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=1024):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        text = f\"[CLS] {text} [SEP]\"\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length' if self.max_length else False,\n",
    "            return_tensors=None,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class AdvancedTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        gradient_accumulation_steps=4,\n",
    "        model_save_path='best_model.pth',\n",
    "        fp16=True\n",
    "    ):\n",
    "        self.model = model\n",
    "      \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "            self.model = nn.DataParallel(model)\n",
    "        self.model.to(device)  # Ensure model is on device after DataParallel\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.scaler = GradScaler() if fp16 else None\n",
    "        self.model_save_path = model_save_path\n",
    "        self.best_metrics = {\n",
    "            'avg_score': 0,\n",
    "            'f1': 0,\n",
    "            'fp_score': 0,\n",
    "            'ap': 0,\n",
    "            'auc': 0\n",
    "        }\n",
    "        self.fp16 = fp16\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(self.train_dataloader, desc=\"Training\")\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "\n",
    "            # Clear gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if self.fp16:\n",
    "                with autocast():\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    # Ensure loss is scalar by taking mean across GPUs\n",
    "                    loss = outputs.loss.mean() / self.gradient_accumulation_steps\n",
    "                \n",
    "                # Scale and backward\n",
    "                self.scaler.scale(loss).backward()\n",
    "\n",
    "                if (i + 1) % self.gradient_accumulation_steps == 0:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            else:\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                # Ensure loss is scalar by taking mean across GPUs\n",
    "                loss = outputs.loss.mean() / self.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "\n",
    "                if (i + 1) % self.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * self.gradient_accumulation_steps\n",
    "            progress_bar.set_postfix({'loss': f'{total_loss/(i+1):.4f}'})\n",
    "            \n",
    "            del outputs, loss\n",
    "            if (i + 1) % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        return total_loss / len(self.train_dataloader)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_dataloader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                if self.fp16:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                \n",
    "                # Take mean of loss across GPUs\n",
    "                loss = outputs.loss.mean()\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Handle logits from DataParallel\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "                preds = (probs > 0.5).long()\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "                del outputs, probs, preds\n",
    "\n",
    "        metrics = self.compute_metrics(all_labels, all_preds, all_probs)\n",
    "        metrics['val_loss'] = total_val_loss / len(self.val_dataloader)\n",
    "        \n",
    "        if metrics['avg_score'] > self.best_metrics['avg_score']:\n",
    "            self.best_metrics = metrics\n",
    "            self.save_model()\n",
    "            print(f\"\\nNew best model saved! Metrics:\")\n",
    "            for k, v in metrics.items():\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def compute_metrics(self, labels, preds, probs):\n",
    "        f1 = f1_score(labels, preds)\n",
    "        fp_score = 1 - ((np.array(preds) > np.array(labels)).sum() / len(labels))\n",
    "        ap = average_precision_score(labels, probs)\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "        avg_score = (f1 + fp_score + ap ) /3\n",
    "\n",
    "        return {\n",
    "            'avg_score': avg_score,\n",
    "            'f1': f1,\n",
    "            'fp_score': fp_score,\n",
    "            'ap': ap,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "    def save_model(self):\n",
    "        # Save the underlying model without DataParallel wrapper\n",
    "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        torch.save({\n",
    "            'model_state_dict': model_to_save.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_metrics': self.best_metrics\n",
    "        }, self.model_save_path)\n",
    "\n",
    "def create_balanced_sample(neg_samples, pos_samples, sample_size=None):\n",
    "    \"\"\"Create a balanced dataset using random sampling\"\"\"\n",
    "    if sample_size is None:\n",
    "        sample_size = min(len(neg_samples), len(pos_samples))\n",
    "    \n",
    "    neg_indices = np.random.choice(len(neg_samples), sample_size, replace=False)\n",
    "    pos_indices = np.random.choice(len(pos_samples), sample_size, replace=False)\n",
    "    \n",
    "    sampled_neg = [neg_samples[i] for i in neg_indices]\n",
    "    sampled_pos = [pos_samples[i] for i in pos_indices]\n",
    "    \n",
    "    return sampled_neg, sampled_pos\n",
    "\n",
    "def main():\n",
    "    # Set up multi-GPU environment\n",
    "    torch.cuda.empty_cache()\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {n_gpu}\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model_path = 'deberta-v3-large-hf-weights'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Load pre-trained weights\n",
    "    state_dict = torch.load('deberta-large-ls03-ctx1024.pth', map_location=device)\n",
    "    \n",
    "    # Initialize model with optimal configuration\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=2,\n",
    "        state_dict=state_dict,\n",
    "        output_hidden_states=True,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        hidden_dropout_prob=0.1\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    with open('data/train_neg_list.pickle', 'rb') as f:\n",
    "        neg_samples = pickle.load(f)\n",
    "    with open('data/train_pos_list.pickle', 'rb') as f:\n",
    "        pos_samples = pickle.load(f)\n",
    "    \n",
    "    # Create balanced dataset\n",
    "    sampled_neg, sampled_pos = create_balanced_sample(neg_samples, pos_samples)\n",
    "    texts = sampled_neg + sampled_pos\n",
    "    labels = [0] * len(sampled_neg) + [1] * len(sampled_pos)\n",
    "\n",
    "    # Split data\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.05, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    # Adjust batch size for multi-GPU training (multiply by number of GPUs)\n",
    "    per_gpu_batch_size = 8\n",
    "    total_train_batch_size = per_gpu_batch_size * n_gpu\n",
    "    total_eval_batch_size = per_gpu_batch_size * 2 * n_gpu\n",
    "\n",
    "    # Create dataloaders with adjusted batch sizes\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=total_train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=total_eval_batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=0.01)\n",
    "    \n",
    "    # Calculate steps for one epoch\n",
    "    num_training_steps = len(train_dataloader) \n",
    "    num_warmup_steps = num_training_steps // 10\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = AdvancedTrainer(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        gradient_accumulation_steps=4,\n",
    "        model_save_path='best_model.pth'\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(\"\\nEpoch 1/1\")\n",
    "    train_loss = trainer.train_epoch()\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    metrics = trainer.evaluate()\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21662a39-3fff-424e-9e05-aff48e145a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
